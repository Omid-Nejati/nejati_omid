<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <style> 
  body {
	background-image: url("Doc/intelligace.jpg");
  	background-repeat: no-repeat;
 	background-attachment: fixed;  
  	background-size: 100% 200px;
	}
  </style>  
</head>
	
  <div>
	 
  <h2 style="text-align:center; margin-top: 50px;font-size:30px">Omid Nejati Manzari</h2>
  <hr style="width:75%;height:5px;border-width:200;color:red;background-color:red">
  <p style="text-align:center; margin-bottom: 50px; font-size:20px"> Iran University of Science and Technology, Tehran, Iran.</p>
  </p>
  </div>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:6%;width:63%;text-align: justify">
              <p style="text-align:center">
                <name>Omid Nejati Manzari</name>
              </p>
			  <p>I am a research assistant at the Computer Vision Center at the Iran University of Science & Technology.
				  I started  master in Electrical Engineering in 2018,  which is being conducted under the supervision
				  of Prof. <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar 
				  Baradaran Shokouhi</a>. My long-term research goal is to enable AI agents to explain phenomena beyond low-level statistics
				  of observable data.
              </p>

		    <p> <strong>Email: omid.nejaty@gmail.com</strong></p>
              </p>
		
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
			  <img width=100% src="Doc/profile_pic.jpg">

            </td>
           </tr> 

	  <table width="100%">
		<tr><td>
			<heading>Researh Interest</heading>
			<ul>
			<li> Computer Vision</li>
			<li> Machine Learning</li>
			<li> Medical Image Processing</li>
			</ul>
		  </td></tr>
</p>
        </tbody></table>
        <table style="width:100%"><tbody>
            <tr>
            <td style="width:100%">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:120%;margin-left:auto"><tbody>

          <tr onmouseout="nightsight_stop_()" onmouseover="nightsight_start_()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_'><img src='Doc/iccke1.png'></div>
                <img src='Doc/iccke1.png'>

              </div>
              <script type="text/javascript">
                function nightsight_start_() {
                  document.getElementById('nightsight_image_').style.opacity = "1";
                }
                function nightsight_stop_() {
                  document.getElementById('nightsight_image_').style.opacity = "0";
                }
                nightsight_stop_()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
                <papertitle> A Robust Network for Embedded Traffic Sign Recognition.</papertitle>
              </a>
              <br>
              <strong>Omid Nejati Manzari</strong>,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B Shokouhi</a>
              <br>
			  <em>International Conference on Computer and Knowledge Engineering </em>, 2021

              <p>In This paper proposes a network that uses
			residual blocks in the network to obtain a top-1 accuracy of 99.51
			for the German traffic sign recognition benchmark. The number
			of parameters is ∼430,000, which is ∼32x fewer than the state-ofthe-art. Experiments have been performed to show the network's
			resistance to destructive factors and its comprehensiveness in the
			application of traffic sign recognition. These tests show that it is a
			comprehensive and robust network for the recognition of traffic
			signs.</p>
            </td>
          </tr>
          
	<tr onmouseout="nightsight_stop_()" onmouseover="nightsight_start_()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_'><img src='Doc/ICCK_2022.png'></div>
                <img src='Doc/ICCK_2022.png'>

              </div>
              <script type="text/javascript">
                function nightsight_start_() {
                  document.getElementById('nightsight_image_').style.opacity = "1";
                }
                function nightsight_stop_() {
                  document.getElementById('nightsight_image_').style.opacity = "0";
                }
                nightsight_stop_()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
                <papertitle> Pyramid Transformer for Traffic Sign Detection.</papertitle>
              </a>
              <br>
              <strong>Omid Nejati Manzari</strong>,
			  Amin Boudesh,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B Shokouhi</a>
              <br>
			  <em>International Conference on Computer and Knowledge Engineering </em>, 2022

              <p>We observed that
			vanilla ViT could not yield satisfactory results in traffic
			sign detection because the overall size of the datasets
			is very small and the class distribution of traffic signs
			is extremely unbalanced. To overcome this problem, a
			novel Pyramid Transformer with locality mechanisms
			is proposed in this paper. Specifically, Pyramid Transformer has several spatial pyramid reduction layers to
			shrink and embed the input image into tokens with rich
			multi-scale context by using atrous convolutions. Moreover, it inherits an intrinsic scale invariance inductive
			bias and is able to learn local feature representation
			for objects at various scales, thereby enhancing the
			network robustness against the size discrepancy of
			traffic signs. The experiments are conducted on the
			German Traffic Sign Detection Benchmark (GTSDB).
			The results demonstrate the superiority of the proposed model in the traffic sign detection tasks. More
			specifically, Pyramid Transformer achieves 77.8% mAP
			on GTSDB when applied to the Cascade RCNN as
			the backbone, which surpasses most well-known and
			widely-used state-of-the-art models.</p>
            </td>
          </tr>
          
         </tr>
       </tbody></table>
        <table style="width:100%"><tbody>
            <tr>
            <td style="width:100%;">
		    	<br><br>
              <heading>Preprints</heading>
		    	    <br><br><br>
			  </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:120%;margin-left:auto"><tbody>

          <tr onmouseout="nightsight_stop_()" onmouseover="nightsight_start_()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_'><img src='Doc/letter.png'></div>
                <img src='Doc/letter.png'>

              </div>
              <script type="text/javascript">
                function nightsight_start_() {
                  document.getElementById('nightsight_image_').style.opacity = "1";
                }
                function nightsight_stop_() {
                  document.getElementById('nightsight_image_').style.opacity = "0";
                }
                nightsight_stop_()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">

                <papertitle> Robust Transformer with Locality Inductive Bias and Feature Normalization.</papertitle>
			  <br>
              <strong>Omid Nejati Manzari</strong>,
			   Hossein Kashiani,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B. Shokouhi</a>

              <p>In this paper, we explore
					the robustness of vision transformers against adversarial
					perturbations and try to enhance their robustness/accuracy
					trade-off in white box attack settings. To this end, we
					propose Locality iN Locality (LNL) transformer model. We
					prove that the locality introduction to LNL contributes to
					the robustness performance since it aggregates local
					information such as lines, edges, shapes, and even objects.
					In addition, to further improve the robustness performance.</p>
            </td>
          </tr>
		
	<tr onmouseout="nightsight_stop_()" onmouseover="nightsight_start_()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_'><img src='Doc/MedViT1.png'></div>
                <img src='Doc/MedVit.png'>

              </div>
              <script type="text/javascript">
                function nightsight_start_() {
                  document.getElementById('nightsight_image_').style.opacity = "1";
                }
                function nightsight_stop_() {
                  document.getElementById('nightsight_image_').style.opacity = "0";
                }
                nightsight_stop_()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">

                <papertitle> MedViT: A Robust Vision Transformer for Generalized Medical Image Classification.</papertitle>
			  <br>
              <strong>Omid Nejati Manzari</strong>,
		    	   Hamid Ahmadabadi,
			   Hossein Kashiani,
			  <a href="https://scholar.google.ca/citations?hl=en&user=sMPEoRcAAAAJ&view_op=list_works&sortby=pubdate">Shahriar B. Shokouhi</a>
		    	

              <p>In this study, we propose a highly robust yet efficient
			CNN-Transformer hybrid model which is equipped with the locality of CNNs as well as the global
			connectivity of vision Transformers. To mitigate the high quadratic complexity of the self-attention
			mechanism while jointly attending to information in various representation subspaces, we construct
			our attention mechanism by means of an efficient convolution operation. Moreover, to alleviate the
			fragility of our Transformer against adversarial attacks, we attempt to smooth out various directions
			of the decision boundary. To this end, we change the shape (or style) context of different instances in
			the high-level feature space by permuting the feature mean and variance across different instances.
			With less computational complexity, our proposed hybrid model demonstrates its high robustness
			and generalization ability compared to the state-of-the-art studies on a large-scale collection of
			standardized MedMNIST-2D datasets.</p>
            </td>
          </tr>	  
		
	</tbody></table>
        <table style="width:100%;"><tbody>
            <tr>
            <td style="width:100%;">
              <heading>Projects</heading>
			  <br><br><br><br><br>
			  </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:120%"><tbody>

         <tr onmouseout="nightsight_stop_2()" onmouseover="nightsight_start_2()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_2'><img src='Doc/covid.png'></div>
                <img src='Doc/covid.png'>
              <script type="text/javascript">
                function nightsight_start_2() {
                  document.getElementById('nightsight_image_2').style.opacity = "1";
                }
                function nightsight_stop_2() {
                  document.getElementById('nightsight_image_2').style.opacity = "0";
                }
                nightsight_stop_2()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
                <papertitle>Local Transformer for COVID-19 Detection Using Chest CT Scans.</papertitle>
              </a>


              <p>Convolutional Neural Networks (CNNs) have mainly
								failed to explicitly model long-range dependencies, primarily
								because of their intrinsic locality. To address this issue, Transformers have drawn increasing interest in exploiting long-range
								dependencies among input data. In this study, we aim to enjoy
								the merits of both local and global feature extractions in CNN
								and Transformer architectures. To this end, we go beyond the
								conventional Transformer frameworks and introduce a highly
								efficient Transformer architecture for early diagnosis and 
		      						treatment of COVID-19 patients using CT images. </p>
            </td>		
         </tr>

	<tr onmouseout="nightsight_stop_()" onmouseover="nightsight_start_()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nightsight_image_'><img src='Doc/line_det.gif'></div>
                <img src=''>

              </div>
              <script type="text/javascript">
                function nightsight_start_() {
                  document.getElementById('nightsight_image_').style.opacity = "1";
                }
                function nightsight_stop_() {
                  document.getElementById('nightsight_image_').style.opacity = "0";
                }
                nightsight_stop_()
              </script>
              <br>
		    
              <td style="padding:20px;width:75%;vertical-align:middle;text-align: justify">
                <papertitle>Fast Drivable Area for Autonomous Driving with Line Detection and Deep Neural Network.</papertitle>
              </a>

              <p>In Autonomous cars use images of the road to detect drivable areas,
		     			 identify lanes, objects near the car, and necessary information.
		     			 This information achieved from the road images are used  to  make
		     			 suitable  driving  decisions  for  self-driving  cars. Drivable 
		      			 area detection is a technique that segments the drivable parts of
		      			 roads in the image. Modern methods often consider road detection as
		      			 a pixel by pixel classification task, which is struggling to  solve  the
		      			 problem  of  computational  cost  and  speed.  So  to increase the speed 
		      			 of  performance, we consider the  process of drivable area recognition as
		      			 a row-selection task. In this paper, special rows in the image are selected.
		      			 Then, the boundaries of the drivable area are detected in these rows.</p>
            </td>
          </tr>

	<table width="100%">
	     <tr><td>
		<p align="center">
 		 <a href="https://scholar.google.com/citations?user=UvZmQzIAAAAJ&hl=en">
   		 <img src="Doc/scholar.png" width= "48" height= "48" hspace="10" />
 		 </a>
		 <a href="https://www.researchgate.net/profile/Omid-Nejati-Manzari">
   		 <img src="Doc/researchgate.png" width= "48" height= "48" hspace="10" />
 		 </a>
		 <a href="https://www.linkedin.com/in/omid-nejati-manzari-a22515119/">
   		 <img src="Doc/linkedin.png" width= "48" height= "48" hspace="10" />
 		 </a>
		 <a href="https://orcid.org/0000-0001-5133-3831">
   		 <img src="Doc/orcid.png" width= "48" height= "48" hspace="10" />
 		 </a>
		 <a href="https://www.instagram.com/nejati__omid/">
   		 <img src="Doc/instagram.png" width= "48" height= "48" hspace="10" />
 		 </a>
		</p>
	     <tr><td>
		
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
		      Time saved by <a href="https://jonbarron.info//">this</a> awesome website.
	    </font>
        </p>
        </td>
        </tr>
		
      </td>
    </tr>
  </table>
</body>

</html>
